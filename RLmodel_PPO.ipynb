{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/50, Avg Reward: 0.3884\n",
      "Episode: 2/50, Avg Reward: 0.3605\n",
      "Episode: 3/50, Avg Reward: 0.3848\n",
      "Episode: 4/50, Avg Reward: 0.4060\n",
      "Episode: 5/50, Avg Reward: 0.4087\n",
      "Episode: 6/50, Avg Reward: 0.4069\n",
      "Episode: 7/50, Avg Reward: 0.4085\n",
      "Episode: 8/50, Avg Reward: 0.3645\n",
      "Episode: 9/50, Avg Reward: 0.3369\n",
      "Episode: 10/50, Avg Reward: 0.3714\n",
      "Episode: 11/50, Avg Reward: 0.3289\n",
      "Episode: 12/50, Avg Reward: 0.3766\n",
      "Episode: 13/50, Avg Reward: 0.4130\n",
      "Episode: 14/50, Avg Reward: 0.3757\n",
      "Episode: 15/50, Avg Reward: 0.3538\n",
      "Episode: 16/50, Avg Reward: 0.4013\n",
      "Episode: 17/50, Avg Reward: 0.3347\n",
      "Episode: 18/50, Avg Reward: 0.3657\n",
      "Episode: 19/50, Avg Reward: 0.3566\n",
      "Episode: 20/50, Avg Reward: 0.3759\n",
      "Episode: 21/50, Avg Reward: 0.3339\n",
      "Episode: 22/50, Avg Reward: 0.4141\n",
      "Episode: 23/50, Avg Reward: 0.3572\n",
      "Episode: 24/50, Avg Reward: 0.4076\n",
      "Episode: 25/50, Avg Reward: 0.3556\n",
      "Episode: 26/50, Avg Reward: 0.3544\n",
      "Episode: 27/50, Avg Reward: 0.4473\n",
      "Episode: 28/50, Avg Reward: 0.4230\n",
      "Episode: 29/50, Avg Reward: 0.4402\n",
      "Episode: 30/50, Avg Reward: 0.4085\n",
      "Episode: 31/50, Avg Reward: 0.3819\n",
      "Episode: 32/50, Avg Reward: 0.4356\n",
      "Episode: 33/50, Avg Reward: 0.4566\n",
      "Episode: 34/50, Avg Reward: 0.3176\n",
      "Episode: 35/50, Avg Reward: 0.3726\n",
      "Episode: 36/50, Avg Reward: 0.3218\n",
      "Episode: 37/50, Avg Reward: 0.4335\n",
      "Episode: 38/50, Avg Reward: 0.4148\n",
      "Episode: 39/50, Avg Reward: 0.3772\n",
      "Episode: 40/50, Avg Reward: 0.3571\n",
      "Episode: 41/50, Avg Reward: 0.4119\n",
      "Episode: 42/50, Avg Reward: 0.4023\n",
      "Episode: 43/50, Avg Reward: 0.4088\n",
      "Episode: 44/50, Avg Reward: 0.3637\n",
      "Episode: 45/50, Avg Reward: 0.4034\n",
      "Episode: 46/50, Avg Reward: 0.4579\n",
      "Episode: 47/50, Avg Reward: 0.4048\n",
      "Episode: 48/50, Avg Reward: 0.3435\n",
      "Episode: 49/50, Avg Reward: 0.4139\n",
      "Episode: 50/50, Avg Reward: 0.3829\n",
      "Creating animation...\n",
      "Saving animation (this may take a while)...\n",
      "Animating frame 1/8\n",
      "Animation saved to 'vsl_animation.mp4'\n",
      "Saving GIF animation...\n",
      "GIF animation saved to 'vsl_animation.gif'\n",
      "Continuous VSL implementation completed. Results and animations saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.dates import DateFormatter\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Load and prepare the data\n",
    "def load_traffic_data(data_path):\n",
    "    # In a real implementation, you'd load from a file\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "# Define the VSL environment\n",
    "class VSLEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        self.speed_limits = [60, 80, 100, 120, 140]  # Available speed limits in km/h\n",
    "        \n",
    "        # Define state space features\n",
    "        self.state_features = ['vehicle_count', 'average_speed_kmh', 'vehicle_density_vpkm']\n",
    "        \n",
    "        # Normalize the features for better learning\n",
    "        self.feature_means = data[self.state_features].mean()\n",
    "        self.feature_stds = data[self.state_features].std()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return None\n",
    "        \n",
    "        current_data = self.data.iloc[self.current_step][self.state_features]\n",
    "        # Normalize the state\n",
    "        normalized_state = (current_data - self.feature_means) / self.feature_stds\n",
    "        return normalized_state.values.astype(np.float32)  # Ensure float32 type\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return None, 0, True, {}\n",
    "        \n",
    "        # Get current traffic conditions\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Apply the selected speed limit\n",
    "        selected_speed_limit = self.speed_limits[action]\n",
    "        \n",
    "        # Advance to the next step\n",
    "        self.current_step += 1\n",
    "        next_data = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Calculate reward based on traffic efficiency and safety\n",
    "        reward = self._calculate_reward(current_data, next_data, selected_speed_limit)\n",
    "        \n",
    "        # Get new state\n",
    "        new_state = self._get_state()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        return new_state, reward, done, {'speed_limit': selected_speed_limit}\n",
    "    \n",
    "    def _calculate_reward(self, current_data, next_data, speed_limit):\n",
    "        # Traffic efficiency component: Reward higher flow rates\n",
    "        flow_reward = next_data['flow_rate_vph'] / 1000  # Normalize\n",
    "        \n",
    "        # Safety component: Penalize if speed is much higher than the limit\n",
    "        speed_compliance = max(0, 1 - max(0, next_data['average_speed_kmh'] - speed_limit) / 50)\n",
    "        \n",
    "        # Stability component: Reward lower variations in speed\n",
    "        speed_stability = 1 / (1 + abs(next_data['average_speed_kmh'] - current_data['average_speed_kmh']))\n",
    "        \n",
    "        # Occupancy component: Reward balanced lane utilization\n",
    "        occupancy_vars = np.var([\n",
    "            next_data['occupancy_lane_1'], \n",
    "            next_data['occupancy_lane_2'], \n",
    "            next_data['occupancy_lane_3']\n",
    "        ])\n",
    "        occupancy_balance = 1 / (1 + occupancy_vars/1000)\n",
    "        \n",
    "        # Combined reward\n",
    "        reward = (0.4 * flow_reward + \n",
    "                  0.3 * speed_compliance + \n",
    "                  0.2 * speed_stability + \n",
    "                  0.1 * occupancy_balance)\n",
    "        \n",
    "        return float(reward)  # Ensure float type\n",
    "\n",
    "# Implement Deep Q-Network agent\n",
    "\n",
    "# Previous VSLEnvironment remains the same as in the original code\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.0003\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon_clip = 0.2  # Clipping parameter\n",
    "        self.c1 = 1.0  # Value loss coefficient\n",
    "        self.c2 = 0.01  # Entropy coefficient\n",
    "        self.epochs = 3  # Number of epochs per update\n",
    "        \n",
    "        # Memory for storing experiences\n",
    "        self.memory = []\n",
    "        \n",
    "        # Actor-Critic Network\n",
    "        self.actor = self._build_actor_network()\n",
    "        self.critic = self._build_critic_network()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.critic_optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "    \n",
    "    def _build_actor_network(self):\n",
    "        # Actor network for policy (action probabilities)\n",
    "        inputs = layers.Input(shape=(self.state_size,))\n",
    "        x = layers.Dense(64, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Output layer with softmax for action probabilities\n",
    "        outputs = layers.Dense(self.action_size, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def _build_critic_network(self):\n",
    "        # Critic network for value function estimation\n",
    "        inputs = layers.Input(shape=(self.state_size,))\n",
    "        x = layers.Dense(64, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Output a single value\n",
    "        outputs = layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done, old_prob):\n",
    "        # Store experiences with additional information for PPO\n",
    "        self.memory.append((state, action, reward, next_state, done, old_prob))\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Predict action probabilities\n",
    "        state = np.array([state], dtype=np.float32)\n",
    "        action_probs = self.actor.predict(state, verbose=0)[0]\n",
    "        \n",
    "        # Sample action based on probabilities\n",
    "        action = np.random.choice(self.action_size, p=action_probs)\n",
    "        \n",
    "        # Get log probability of the chosen action\n",
    "        log_prob = np.log(action_probs[action])\n",
    "        \n",
    "        return action  # Return only the action index\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Prepare batches\n",
    "        states = np.array([mem[0] for mem in self.memory], dtype=np.float32)\n",
    "        actions = np.array([mem[1] for mem in self.memory])\n",
    "        rewards = np.array([mem[2] for mem in self.memory], dtype=np.float32)\n",
    "        next_states = np.array([mem[3] for mem in self.memory], dtype=np.float32)\n",
    "        dones = np.array([mem[4] for mem in self.memory])\n",
    "        old_log_probs = np.array([mem[5] for mem in self.memory])\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns = np.zeros_like(rewards)\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        values = self.critic.predict(states, verbose=0).flatten()\n",
    "        next_values = self.critic.predict(next_states, verbose=0).flatten()\n",
    "        \n",
    "        for t in range(len(rewards)):\n",
    "            # Compute TD error\n",
    "            delta = rewards[t] + self.gamma * next_values[t] * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            # Compute advantage\n",
    "            advantages[t] = delta\n",
    "            \n",
    "            # Compute returns (GAE)\n",
    "            returns[t] = rewards[t] + self.gamma * next_values[t] * (1 - dones[t])\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "        \n",
    "        # Perform PPO update using a custom training function\n",
    "        self._ppo_update(states, actions, advantages, returns, old_log_probs)\n",
    "        \n",
    "        # Clear memory after training\n",
    "        self.memory.clear()\n",
    "    \n",
    "    def _ppo_update(self, states, actions, advantages, returns, old_log_probs):\n",
    "        # Use GradientTape for explicit gradient computation\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            # Actor network computations\n",
    "            current_probs = self.actor(states)\n",
    "            \n",
    "            # Compute log probabilities\n",
    "            indices = tf.range(len(actions))\n",
    "            action_masks = tf.one_hot(actions, self.action_size)\n",
    "            current_log_probs = tf.math.log(tf.reduce_sum(current_probs * action_masks, axis=1) + 1e-10)\n",
    "            \n",
    "            # Compute probability ratio\n",
    "            ratio = tf.exp(current_log_probs - tf.cast(old_log_probs, tf.float32))\n",
    "            \n",
    "            # Clipped surrogate objective\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = tf.clip_by_value(ratio, 1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages\n",
    "            policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "            \n",
    "            # Critic (value) loss\n",
    "            critic_values = tf.squeeze(self.critic(states))\n",
    "            value_loss = tf.reduce_mean(tf.square(returns - critic_values))\n",
    "            \n",
    "            # Entropy loss\n",
    "            entropy_loss = -tf.reduce_mean(current_probs * tf.math.log(current_probs + 1e-10))\n",
    "            \n",
    "            # Composite losses\n",
    "            total_actor_loss = policy_loss - self.c2 * entropy_loss\n",
    "            total_critic_loss = self.c1 * value_loss\n",
    "        \n",
    "        # Compute gradients\n",
    "        actor_grads = actor_tape.gradient(total_actor_loss, self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(total_critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        max_grad_norm = 1.0\n",
    "        actor_grads = [tf.clip_by_norm(grad, max_grad_norm) for grad in actor_grads]\n",
    "        critic_grads = [tf.clip_by_norm(grad, max_grad_norm) for grad in critic_grads]\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        \n",
    "        # Clear memory after training\n",
    "        self.memory.clear()\n",
    "    \n",
    "    def save(self, actor_path, critic_path):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "    \n",
    "    def load(self, actor_path, critic_path):\n",
    "        self.actor.load_weights(actor_path)\n",
    "        self.critic.load_weights(critic_path)\n",
    "\n",
    "# Modify train_vsl_agent function\n",
    "def train_vsl_agent(data, episodes=100, batch_size=32):\n",
    "    env = VSLEnvironment(data)\n",
    "    state_size = len(env.state_features)\n",
    "    action_size = len(env.speed_limits)\n",
    "    agent = PPOAgent(state_size, action_size)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        'episode': [],\n",
    "        'average_reward': [],\n",
    "        'speed_limits': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        speed_limits_applied = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Act with action and log probability\n",
    "            action, log_prob = agent.act(state), 0.0  # Default log_prob if not used\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if next_state is None:  # End of data\n",
    "                break\n",
    "                \n",
    "            speed_limits_applied.append(env.speed_limits[action])\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experience with log probability\n",
    "            agent.remember(state, action, reward, next_state, done, log_prob)\n",
    "            state = next_state\n",
    "        \n",
    "        # Train the agent on collected experiences\n",
    "        if len(agent.memory) >= batch_size:\n",
    "            agent.train(batch_size)\n",
    "        \n",
    "        # Store results\n",
    "        results['episode'].append(episode)\n",
    "        results['average_reward'].append(total_reward / len(data))\n",
    "        results['speed_limits'].append(speed_limits_applied)\n",
    "        \n",
    "        print(f\"Episode: {episode+1}/{episodes}, Avg Reward: {total_reward/len(data):.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent.save(\"models/ppo_actor_model.weights.h5\", \"models/ppo_critic_model.weights.h5\")\n",
    "    return results, agent\n",
    "# The rest of the code remains the same as in the original implementation\n",
    "# (load_traffic_data, evaluate_vsl_agent, visualize_results, run_vsl_pipeline)\n",
    "# Main training function\n",
    "\n",
    "# Evaluate the trained agent\n",
    "def evaluate_vsl_agent(data, agent):\n",
    "    env = VSLEnvironment(data)\n",
    "    state = env.reset()\n",
    "    recommended_speeds = []\n",
    "    actual_speeds = []\n",
    "    flow_rates = []\n",
    "    timestamps = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        speed_limit = env.speed_limits[action]\n",
    "        recommended_speeds.append(speed_limit)\n",
    "        \n",
    "        # Store actual values for comparison\n",
    "        current_data = data.iloc[env.current_step]\n",
    "        actual_speeds.append(current_data['average_speed_kmh'])\n",
    "        flow_rates.append(current_data['flow_rate_vph'])\n",
    "        timestamps.append(current_data['timestamp'])\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if next_state is None:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    return timestamps, recommended_speeds, actual_speeds, flow_rates\n",
    "\n",
    "# Modified visualization function with animation\n",
    "def visualize_results_with_animation(timestamps, recommended_speeds, actual_speeds, flow_rates):\n",
    "    # Static visualization (same as before)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot recommended speed limits and actual speeds\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(timestamps, recommended_speeds, 'b-', label='Recommended Speed Limit')\n",
    "    plt.plot(timestamps, actual_speeds, 'r-', label='Actual Average Speed')\n",
    "    plt.ylabel('Speed (km/h)')\n",
    "    plt.title('Continuous VSL Recommendations vs. Actual Speeds')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot flow rates\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(timestamps, flow_rates, 'g-', label='Flow Rate')\n",
    "    plt.ylabel('Flow Rate (vph)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/vsl_results2.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create animated visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    fig.suptitle('Variable Speed Limit System Animation', fontsize=16)\n",
    "    \n",
    "    # Format dates on x-axis\n",
    "    date_format = DateFormatter('%H:%M')\n",
    "    \n",
    "    # Empty lines for initialization\n",
    "    speed_limit_line, = ax1.plot([], [], 'b-', linewidth=2, label='Recommended Speed Limit')\n",
    "    actual_speed_line, = ax1.plot([], [], 'r-', linewidth=2, label='Actual Average Speed')\n",
    "    flow_rate_line, = ax2.plot([], [], 'g-', linewidth=2, label='Flow Rate')\n",
    "    \n",
    "    # Adding a vertical line to track current time\n",
    "    current_time_line1 = ax1.axvline(x=timestamps[0], color='k', linestyle='--', alpha=0.5)\n",
    "    current_time_line2 = ax2.axvline(x=timestamps[0], color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Adding a text annotation for the current speed limit\n",
    "    speed_text = ax1.text(0.02, 0.92, '', transform=ax1.transAxes, fontsize=12, \n",
    "                         bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Set axis labels and grid\n",
    "    ax1.set_ylabel('Speed (km/h)')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    ax2.set_ylabel('Flow Rate (vph)')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to display time correctly\n",
    "    ax1.xaxis.set_major_formatter(date_format)\n",
    "    ax2.xaxis.set_major_formatter(date_format)\n",
    "    \n",
    "    # Set fixed y-axis limits for better visualization\n",
    "    ax1.set_ylim(0, max(max(recommended_speeds), max(actual_speeds)) * 1.1)\n",
    "    ax2.set_ylim(0, max(flow_rates) * 1.1)\n",
    "    \n",
    "    # Set fixed x-axis limits\n",
    "    ax1.set_xlim(timestamps[0], timestamps[-1])\n",
    "    ax2.set_xlim(timestamps[0], timestamps[-1])\n",
    "    \n",
    "    # Create a counter for animation frames\n",
    "    frame_count = len(timestamps)\n",
    "    \n",
    "    # Initialize the animation\n",
    "    def init():\n",
    "        speed_limit_line.set_data([], [])\n",
    "        actual_speed_line.set_data([], [])\n",
    "        flow_rate_line.set_data([], [])\n",
    "        speed_text.set_text('')\n",
    "        return speed_limit_line, actual_speed_line, flow_rate_line, current_time_line1, current_time_line2, speed_text\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(i):\n",
    "        # Display progress for longer animations\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Animating frame {i+1}/{frame_count}\")\n",
    "        \n",
    "        # Number of points to show (growing window)\n",
    "        window_size = min(i + 1, len(timestamps))\n",
    "        end_idx = i + 1\n",
    "        \n",
    "        # Update data\n",
    "        speed_limit_line.set_data(timestamps[:end_idx], recommended_speeds[:end_idx])\n",
    "        actual_speed_line.set_data(timestamps[:end_idx], actual_speeds[:end_idx])\n",
    "        flow_rate_line.set_data(timestamps[:end_idx], flow_rates[:end_idx])\n",
    "        \n",
    "        # Update current time marker\n",
    "        current_time = timestamps[i]\n",
    "        current_time_line1.set_xdata([current_time, current_time])\n",
    "        current_time_line2.set_xdata([current_time, current_time])\n",
    "        \n",
    "        # Update speed limit text\n",
    "        speed_text.set_text(f'Current Speed Limit: {recommended_speeds[i]:.1f} km/h')\n",
    "        \n",
    "        return speed_limit_line, actual_speed_line, flow_rate_line, current_time_line1, current_time_line2, speed_text\n",
    "    \n",
    "    # Create animation\n",
    "    print(\"Creating animation...\")\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=frame_count, \n",
    "                                  interval=200, blit=True)\n",
    "    \n",
    "    # Save animation\n",
    "    print(\"Saving animation (this may take a while)...\")\n",
    "    # Use a lower dpi for faster rendering during development, increase for final version\n",
    "    anim.save('output/vsl_animation.mp4', writer='ffmpeg', fps=10, dpi=100)\n",
    "    print(\"Animation saved to 'vsl_animation.mp4'\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Additionally create a simplified GIF version (easier to view)\n",
    "    # Use a subset of frames for a smaller file size\n",
    "    frame_subset = max(1, frame_count // 100)  # Skip frames for efficiency\n",
    "    \n",
    "    fig, ax = plt.figure(figsize=(10, 6)), plt.axes()\n",
    "    ax.set_title('VSL Speed Limit Animation')\n",
    "    ax.set_ylabel('Speed (km/h)')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    line, = ax.plot([], [], 'b-', linewidth=3, label='Speed Limit')\n",
    "    time_text = ax.text(0.02, 0.92, '', transform=ax.transAxes, fontsize=12,\n",
    "                       bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlim(timestamps[0], timestamps[-1])\n",
    "    ax.set_ylim(min(recommended_speeds) * 0.9, max(recommended_speeds) * 1.1)\n",
    "    ax.xaxis.set_major_formatter(date_format)\n",
    "    ax.legend()\n",
    "    \n",
    "    def init_gif():\n",
    "        line.set_data([], [])\n",
    "        time_text.set_text('')\n",
    "        return line, time_text\n",
    "    \n",
    "    def animate_gif(i):\n",
    "        i = i * frame_subset  # Skip frames\n",
    "        idx = min(i, len(timestamps)-1)\n",
    "        line.set_data(timestamps[:idx+1], recommended_speeds[:idx+1])\n",
    "        time_text.set_text(f'Time: {timestamps[idx].strftime(\"%H:%M\")}\\nSpeed: {recommended_speeds[idx]:.1f} km/h')\n",
    "        return line, time_text\n",
    "    \n",
    "    gif_frames = len(timestamps) // frame_subset\n",
    "    anim_gif = animation.FuncAnimation(fig, animate_gif, init_func=init_gif, \n",
    "                                     frames=gif_frames, interval=100, blit=True)\n",
    "    \n",
    "    print(\"Saving GIF animation...\")\n",
    "    anim_gif.save('output/vsl_animation.gif', writer='pillow', fps=5, dpi=80)\n",
    "    print(\"GIF animation saved to 'vsl_animation.gif'\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Run the complete pipeline\n",
    "def run_continuous_vsl_pipeline():\n",
    "    # Load data\n",
    "    data = load_traffic_data(\"traffic_data_2.csv\")\n",
    "    \n",
    "    # Train the agent\n",
    "    results, agent = train_vsl_agent(data, episodes=50)\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    timestamps, recommended_speeds, actual_speeds, flow_rates = evaluate_vsl_agent(data, agent)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results_with_animation(timestamps, recommended_speeds, actual_speeds, flow_rates)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_continuous_vsl_pipeline()\n",
    "    print(\"Continuous VSL implementation completed. Results and animations saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with avg reward: 0.4061\n",
      "Episode: 1/50, Avg Reward: 0.4061\n",
      "New best model saved with avg reward: 0.4165\n",
      "Episode: 2/50, Avg Reward: 0.4165\n",
      "Episode: 3/50, Avg Reward: 0.3815\n",
      "Episode: 4/50, Avg Reward: 0.2999\n",
      "New best model saved with avg reward: 0.4423\n",
      "Episode: 5/50, Avg Reward: 0.4423\n",
      "Episode: 6/50, Avg Reward: 0.3358\n",
      "Episode: 7/50, Avg Reward: 0.3376\n",
      "Episode: 8/50, Avg Reward: 0.4336\n",
      "Episode: 9/50, Avg Reward: 0.3625\n",
      "Episode: 10/50, Avg Reward: 0.3842\n",
      "Episode: 11/50, Avg Reward: 0.2924\n",
      "Episode: 12/50, Avg Reward: 0.3376\n",
      "Episode: 13/50, Avg Reward: 0.3223\n",
      "Episode: 14/50, Avg Reward: 0.3978\n",
      "Episode: 15/50, Avg Reward: 0.3456\n",
      "Episode: 16/50, Avg Reward: 0.4023\n",
      "Episode: 17/50, Avg Reward: 0.3699\n",
      "Episode: 18/50, Avg Reward: 0.3585\n",
      "Episode: 19/50, Avg Reward: 0.3785\n",
      "Episode: 20/50, Avg Reward: 0.3606\n",
      "Episode: 21/50, Avg Reward: 0.3933\n",
      "Episode: 22/50, Avg Reward: 0.3394\n",
      "Episode: 23/50, Avg Reward: 0.3776\n",
      "New best model saved with avg reward: 0.4566\n",
      "Episode: 24/50, Avg Reward: 0.4566\n",
      "Episode: 25/50, Avg Reward: 0.3627\n",
      "Episode: 26/50, Avg Reward: 0.4259\n",
      "Episode: 27/50, Avg Reward: 0.3390\n",
      "Episode: 28/50, Avg Reward: 0.2976\n",
      "Episode: 29/50, Avg Reward: 0.2964\n",
      "Episode: 30/50, Avg Reward: 0.4172\n",
      "Episode: 31/50, Avg Reward: 0.3245\n",
      "Episode: 32/50, Avg Reward: 0.4429\n",
      "Episode: 33/50, Avg Reward: 0.3521\n",
      "Episode: 34/50, Avg Reward: 0.3591\n",
      "Episode: 35/50, Avg Reward: 0.3629\n",
      "Episode: 36/50, Avg Reward: 0.3151\n",
      "Episode: 37/50, Avg Reward: 0.3835\n",
      "Episode: 38/50, Avg Reward: 0.3900\n",
      "Episode: 39/50, Avg Reward: 0.3514\n",
      "Episode: 40/50, Avg Reward: 0.3555\n",
      "Episode: 41/50, Avg Reward: 0.3462\n",
      "Episode: 42/50, Avg Reward: 0.3214\n",
      "Episode: 43/50, Avg Reward: 0.3106\n",
      "Episode: 44/50, Avg Reward: 0.3909\n",
      "Episode: 45/50, Avg Reward: 0.4061\n",
      "Episode: 46/50, Avg Reward: 0.3128\n",
      "Episode: 47/50, Avg Reward: 0.4316\n",
      "Episode: 48/50, Avg Reward: 0.3839\n",
      "Episode: 49/50, Avg Reward: 0.2927\n",
      "Episode: 50/50, Avg Reward: 0.3357\n",
      "Creating speed limit panel animation...\n",
      "Saving animation to output/vsl_speed_panel.mp4...\n",
      "Rendering frame 1/8\n",
      "Rendering frame 1/8\n",
      "Animation saved to output/vsl_speed_panel.mp4\n",
      "Saving GIF animation to output/vsl_speed_panel.gif...\n",
      "Rendering frame 1/8\n",
      "Rendering frame 1/8\n",
      "GIF animation saved to output/vsl_speed_panel.gif\n",
      "Continuous VSL implementation completed. Results and animations saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.font_manager as fm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Continuous Variable Speed Limit Environment\n",
    "class ContinuousVSLEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Define continuous action space limits\n",
    "        self.min_speed_limit = 50  # Minimum speed limit\n",
    "        self.max_speed_limit = 140  # Maximum speed limit\n",
    "        \n",
    "        # Define state space features\n",
    "        self.state_features = ['vehicle_count', 'average_speed_kmh', 'vehicle_density_vpkm']\n",
    "        \n",
    "        # Normalize the features for better learning\n",
    "        self.feature_means = data[self.state_features].mean()\n",
    "        self.feature_stds = data[self.state_features].std()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return None\n",
    "        \n",
    "        current_data = self.data.iloc[self.current_step][self.state_features]\n",
    "        # Normalize the state\n",
    "        normalized_state = (current_data - self.feature_means) / self.feature_stds\n",
    "        return normalized_state.values.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return None, 0, True, {}\n",
    "        \n",
    "        # Scale continuous action to speed limit range\n",
    "        scaled_speed_limit = self.min_speed_limit + (self.max_speed_limit - self.min_speed_limit) * action\n",
    "        \n",
    "        # Get current traffic conditions\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Advance to the next step\n",
    "        self.current_step += 1\n",
    "        next_data = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Calculate reward based on traffic efficiency and safety\n",
    "        reward = self._calculate_reward(current_data, next_data, scaled_speed_limit)\n",
    "        \n",
    "        # Get new state\n",
    "        new_state = self._get_state()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        return new_state, reward, done, {'speed_limit': scaled_speed_limit}\n",
    "    \n",
    "    def _calculate_reward(self, current_data, next_data, speed_limit):\n",
    "        # Traffic efficiency component: Reward higher flow rates\n",
    "        flow_reward = next_data['flow_rate_vph'] / 1000  # Normalize\n",
    "        \n",
    "        # Safety component: Penalize if speed is much higher than the limit\n",
    "        speed_compliance = max(0, 1 - max(0, next_data['average_speed_kmh'] - speed_limit) / 50) # 50 is min spped limit\n",
    "        \n",
    "        # Stability component: Reward lower variations in speed\n",
    "        speed_stability = 1 / (1 + abs(next_data['average_speed_kmh'] - current_data['average_speed_kmh']))\n",
    "        \n",
    "        # Occupancy component: Reward balanced lane utilization\n",
    "        occupancy_vars = np.var([\n",
    "            next_data['occupancy_lane_1'], \n",
    "            next_data['occupancy_lane_2'], \n",
    "            next_data['occupancy_lane_3']\n",
    "        ])\n",
    "        occupancy_balance = 1 / (1 + occupancy_vars/1000)\n",
    "        \n",
    "        # Combined reward\n",
    "        reward = (0.4 * flow_reward + \n",
    "                  0.3 * speed_compliance + \n",
    "                  0.2 * speed_stability + \n",
    "                  0.1 * occupancy_balance)\n",
    "        \n",
    "        return float(reward)  # Ensure float type\n",
    "\n",
    "# Continuous PPO Agent\n",
    "class ContinuousPPOAgent:\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "        \n",
    "       # Improved hyperparameters\n",
    "        self.learning_rate = 0.0001  # Lower learning rate for more stable learning\n",
    "        self.gamma = 0.99  # Higher discount factor for better long-term planning\n",
    "        self.epsilon_clip = 0.15  # Adjusted clipping parameter\n",
    "        self.c1 = 0.8  # Value loss coefficient\n",
    "        self.c2 = 0.02  # Entropy coefficient (slightly higher for better exploration)\n",
    "        self.epochs = 5  # More epochs per update\n",
    "        \n",
    "        # Memory with larger capacity\n",
    "        self.memory = []\n",
    "        self.memory_capacity = 10000\n",
    "        \n",
    "        # Actor-Critic Network with improved architecture\n",
    "        self.actor = self._build_actor_network()\n",
    "        self.critic = self._build_critic_network()\n",
    "        \n",
    "        # Optimizers with learning rate decay\n",
    "        self.actor_optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=self.learning_rate,\n",
    "                decay_steps=1000,\n",
    "                decay_rate=0.95\n",
    "            )\n",
    "        )\n",
    "        self.critic_optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=self.learning_rate,\n",
    "                decay_steps=1000,\n",
    "                decay_rate=0.95\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _build_actor_network(self):\n",
    "        # Actor network for policy (continuous action distribution)\n",
    "        inputs = layers.Input(shape=(self.state_size,))\n",
    "        \n",
    "        # Add batch normalization for better training stability\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        \n",
    "        # Wider and deeper network\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "        # Output mean and log std for a Gaussian distribution\n",
    "        mean = layers.Dense(1, activation='tanh')(x)  # Using tanh to bound the output\n",
    "        mean = layers.Lambda(lambda x: x * 0.5 + 0.5)(mean)  # Rescale to [0,1]\n",
    "        \n",
    "        log_std = layers.Dense(1, activation='linear')(x)\n",
    "        # Bound log_std for stability\n",
    "        log_std = layers.Lambda(lambda x: tf.clip_by_value(x, -20, 0))(log_std)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=[mean, log_std])\n",
    "        return model\n",
    "    \n",
    "    def _build_critic_network(self):\n",
    "        # Critic network for value function estimation\n",
    "        inputs = layers.Input(shape=(self.state_size,))\n",
    "        \n",
    "        # Add batch normalization\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        \n",
    "        # Wider and deeper network\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Output a single value\n",
    "        outputs = layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done, old_mean, old_std):\n",
    "        # Store experiences with additional information for PPO\n",
    "        self.memory.append((state, action, reward, next_state, done, old_mean, old_std))\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Predict action distribution parameters\n",
    "        state = np.array([state], dtype=np.float32)\n",
    "        mean, log_std = self.actor.predict(state, verbose=0)\n",
    "        mean, log_std = mean[0][0], log_std[0][0]\n",
    "        \n",
    "        # Sample action from Gaussian distribution\n",
    "        std = np.exp(log_std)\n",
    "        action = np.random.normal(mean, std)\n",
    "        \n",
    "        # Clip action to [0, 1] range for environment\n",
    "        action = np.clip(action, 0, 1)\n",
    "        \n",
    "        return action, mean, std\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Prepare batches\n",
    "        states = np.array([mem[0] for mem in self.memory], dtype=np.float32)\n",
    "        actions = np.array([mem[1] for mem in self.memory], dtype=np.float32)\n",
    "        rewards = np.array([mem[2] for mem in self.memory], dtype=np.float32)\n",
    "        next_states = np.array([mem[3] for mem in self.memory], dtype=np.float32)\n",
    "        dones = np.array([mem[4] for mem in self.memory])\n",
    "        old_means = np.array([mem[5] for mem in self.memory], dtype=np.float32)\n",
    "        old_stds = np.array([mem[6] for mem in self.memory], dtype=np.float32)\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns = np.zeros_like(rewards)\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        \n",
    "        # Compute values\n",
    "        values = self.critic.predict(states, verbose=0).flatten()\n",
    "        next_values = self.critic.predict(next_states, verbose=0).flatten()\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        for t in range(len(rewards)):\n",
    "            # Compute TD error\n",
    "            # Measures how much better an action was compared to the expected return.\n",
    "            delta = rewards[t] + self.gamma * next_values[t] * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            # Compute advantage\n",
    "            advantages[t] = delta\n",
    "            \n",
    "            # Compute returns (GAE)\n",
    "            # Generalized Advantage Estimation\n",
    "            returns[t] = rewards[t] + self.gamma * next_values[t] * (1 - dones[t])\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "        \n",
    "        # Perform PPO update\n",
    "        self._ppo_update(states, actions, advantages, returns, old_means, old_stds)\n",
    "        \n",
    "        # Clear memory after training\n",
    "        self.memory.clear()\n",
    "    \n",
    "    def _ppo_update(self, states, actions, advantages, returns, old_means, old_stds):\n",
    "        # Use GradientTape for explicit gradient computation record for coompute the gradients\n",
    "        # This function updates both the actor (policy) and critic (value function) networks.\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            # Actor network computations\n",
    "            current_means, current_log_stds = self.actor(states)\n",
    "            current_means = tf.squeeze(current_means)# remove unccessary dimensions\n",
    "            current_log_stds = tf.squeeze(current_log_stds)\n",
    "            current_stds = tf.exp(current_log_stds)\n",
    "            \n",
    "            # Compute log probabilities for continuous actions\n",
    "            #log probability density function of a multivariate Gaussian distribution.\n",
    "            log_prob = -0.5 * tf.math.log(2 * np.pi * current_stds**2) - \\\n",
    "                           0.5 * ((actions - current_means) / current_stds)**2\n",
    "            log_prob = tf.reduce_sum(log_prob, axis=-1)\n",
    "            \n",
    "            # Compute old log probabilities\n",
    "            old_log_prob = -0.5 * tf.math.log(2 * np.pi * old_stds**2) - \\\n",
    "                               0.5 * ((actions - old_means) / old_stds)**2\n",
    "            old_log_prob = tf.reduce_sum(old_log_prob, axis=-1)\n",
    "            \n",
    "            # Compute probability ratio\n",
    "            ratio = tf.exp(log_prob - old_log_prob)\n",
    "            \n",
    "            # Clipped surrogate objective\n",
    "    #        PPO Clipping Mechanism:\n",
    "    #       Limits how much the new policy can deviate from the old one.\n",
    "    #       Prevents overly aggressive updates.\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = tf.clip_by_value(ratio, 1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages\n",
    "            policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "            \n",
    "            # Critic (value) loss\n",
    "            # MSE Loss for the Critic Network:\n",
    "            critic_values = tf.squeeze(self.critic(states))\n",
    "            value_loss = tf.reduce_mean(tf.square(returns - critic_values))\n",
    "            \n",
    "            # Entropy loss (encourage exploration)\n",
    "            entropy_loss = tf.reduce_mean(current_log_stds + 0.5 * np.log(2.0 * np.pi * np.e))\n",
    "            \n",
    "            # Composite losses\n",
    "            # Actor loss: Policy loss with entropy regularization.\n",
    "            #Critic loss: Value function loss.\n",
    "            total_actor_loss = policy_loss - self.c2 * entropy_loss\n",
    "            total_critic_loss = self.c1 * value_loss\n",
    "        \n",
    "        # Compute gradients\n",
    "        actor_grads = actor_tape.gradient(total_actor_loss, self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(total_critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        max_grad_norm = 0.5\n",
    "        actor_grads = [tf.clip_by_norm(grad, max_grad_norm) for grad in actor_grads]\n",
    "        critic_grads = [tf.clip_by_norm(grad, max_grad_norm) for grad in critic_grads]\n",
    "        \n",
    "        # Apply gradients\n",
    "        # Updates the actor and critic networks.\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "    \n",
    "    def save(self, actor_path, critic_path):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "    \n",
    "    def load(self, actor_path, critic_path):\n",
    "        self.actor.load_weights(actor_path)\n",
    "        self.critic.load_weights(critic_path)\n",
    "\n",
    "# Load and prepare the data\n",
    "def load_traffic_data(data_path):\n",
    "    # In a real implementation, you'd load from a file\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "# Training function for continuous VSL agent\n",
    "def train_continuous_vsl_agent(data, episodes=100, batch_size=32):\n",
    "    env = ContinuousVSLEnvironment(data)\n",
    "    state_size = len(env.state_features)\n",
    "    agent = ContinuousPPOAgent(state_size)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        'episode': [],\n",
    "        'average_reward': [],\n",
    "        'speed_limits': []\n",
    "    }\n",
    "\n",
    "     # Store best model\n",
    "    best_reward = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        speed_limits_applied = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Act with action and distribution parameters\n",
    "            action, mean, std = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if next_state is None:  # End of data\n",
    "                break\n",
    "                \n",
    "            # Scale action back to speed limit for recording\n",
    "            scaled_speed_limit = env.min_speed_limit + (env.max_speed_limit - env.min_speed_limit) * action\n",
    "            speed_limits_applied.append(scaled_speed_limit)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experience with distribution parameters\n",
    "            agent.remember(state, action, reward, next_state, done, mean, std)\n",
    "            state = next_state\n",
    "        \n",
    "        # Train the agent on collected experiences\n",
    "        if len(agent.memory) >= batch_size:\n",
    "            agent.train(batch_size)\n",
    "        \n",
    "        # Store results\n",
    "        avg_reward = total_reward / len(data)\n",
    "        results['episode'].append(episode)\n",
    "        results['average_reward'].append(avg_reward)\n",
    "        results['speed_limits'].append(speed_limits_applied)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            agent.save(\"models/best_ppo_actor_model.weights.h5\", \"models/best_ppo_critic_model.weights.h5\")\n",
    "            print(f\"New best model saved with avg reward: {avg_reward:.4f}\")\n",
    "        \n",
    "        print(f\"Episode: {episode+1}/{episodes}, Avg Reward: {avg_reward:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent.save(\"models/continuous_ppo_actor_model.weights.h5\", \"models/continuous_ppo_critic_model.weights.h5\")\n",
    "    return results, agent\n",
    "\n",
    "def create_speed_limit_panel_animation(timestamps, recommended_speeds, output_file='output/vsl_speed_panel.mp4'):\n",
    "    \"\"\"\n",
    "    Create an animation of a digital speed limit panel that changes over time.\n",
    "    \n",
    "    Parameters:\n",
    "    timestamps - List of datetime objects\n",
    "    recommended_speeds - List of speed limit values\n",
    "    output_file - Filename for the output animation\n",
    "    \"\"\"\n",
    "    # Round speed limits to whole numbers for display\n",
    "    speeds_rounded = [round(speed) for speed in recommended_speeds]\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=0.9)\n",
    "    \n",
    "    # Turn off the axis\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create a speed limit sign background\n",
    "    def draw_speed_sign(speed):\n",
    "        # Clear the axis for the new frame\n",
    "        ax.clear()\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create the red circle border\n",
    "        circle = Circle((0.5, 0.5), 0.4, facecolor='white', edgecolor='red', linewidth=10, transform=ax.transAxes)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # Add the speed text\n",
    "        ax.text(0.5, 0.5, f\"{speed}\", fontsize=150, ha='center', va='center', \n",
    "                transform=ax.transAxes, fontweight='bold')\n",
    "        \n",
    "        # Add \"km/h\" text below\n",
    "        ax.text(0.5, 0.25, \"km/h\", fontsize=40, ha='center', va='center', \n",
    "                transform=ax.transAxes)\n",
    "        \n",
    "        # Add timestamp\n",
    "        time_idx = min(frame_num, len(timestamps)-1)\n",
    "        current_time = timestamps[time_idx].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ax.text(0.5, 0.9, f\"Time: {current_time}\", fontsize=20, ha='center', va='center',\n",
    "                transform=ax.transAxes)\n",
    "        \n",
    "        # Add variable speed limit label\n",
    "        ax.text(0.5, 0.85, \"VARIABLE SPEED LIMIT\", fontsize=20, ha='center', va='center',\n",
    "                transform=ax.transAxes, fontweight='bold')\n",
    "        \n",
    "    # Initialize frame counter (used in the animation function)\n",
    "    frame_num = 0\n",
    "    \n",
    "    # Animation function\n",
    "    def update_sign(frame):\n",
    "        nonlocal frame_num\n",
    "        frame_num = frame\n",
    "        \n",
    "        # Get current speed (with protection against index errors)\n",
    "        if frame < len(speeds_rounded):\n",
    "            current_speed = speeds_rounded[frame]\n",
    "        else:\n",
    "            current_speed = speeds_rounded[-1]\n",
    "            \n",
    "        # Draw the updated sign\n",
    "        draw_speed_sign(current_speed)\n",
    "        \n",
    "        # Print progress\n",
    "        if frame % 10 == 0:\n",
    "            print(f\"Rendering frame {frame+1}/{len(speeds_rounded)}\")\n",
    "            \n",
    "        return [ax]\n",
    "    \n",
    "    # Create animation\n",
    "    print(\"Creating speed limit panel animation...\")\n",
    "    ani = animation.FuncAnimation(fig, update_sign, frames=len(speeds_rounded),\n",
    "                                  interval=200, blit=False)\n",
    "    \n",
    "    # Save animation\n",
    "    print(f\"Saving animation to {output_file}...\")\n",
    "    ani.save(output_file, writer='ffmpeg', fps=5, dpi=100)\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved to {output_file}\")\n",
    "    \n",
    "    # Create a GIF version as well (typically smaller file size)\n",
    "    gif_output = output_file.replace('.mp4', '.gif')\n",
    "    \n",
    "    # Use fewer frames for the GIF to keep file size reasonable\n",
    "    # We'll sample every nth frame\n",
    "    frame_skip = max(1, len(speeds_rounded) // 50)\n",
    "    frames_to_use = range(0, len(speeds_rounded), frame_skip)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=0.9)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    frame_num = 0\n",
    "    \n",
    "    # Create animation with reduced frames\n",
    "    ani_gif = animation.FuncAnimation(fig, update_sign, frames=frames_to_use,\n",
    "                                     interval=200, blit=False)\n",
    "    \n",
    "    print(f\"Saving GIF animation to {gif_output}...\")\n",
    "    ani_gif.save(gif_output, writer='pillow', fps=2, dpi=80)\n",
    "    plt.close(fig)\n",
    "    print(f\"GIF animation saved to {gif_output}\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "def evaluate_continuous_vsl_agent(data, agent):\n",
    "    env = ContinuousVSLEnvironment(data)\n",
    "    state = env.reset()\n",
    "    recommended_speeds = []\n",
    "    actual_speeds = []\n",
    "    flow_rates = []\n",
    "    timestamps = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _, _ = agent.act(state)\n",
    "        # Scale action to speed limit\n",
    "        speed_limit = env.min_speed_limit + (env.max_speed_limit - env.min_speed_limit) * action\n",
    "        recommended_speeds.append(speed_limit)\n",
    "        \n",
    "        # Store actual values for comparison\n",
    "        current_data = data.iloc[env.current_step]\n",
    "        actual_speeds.append(current_data['average_speed_kmh'])\n",
    "        flow_rates.append(current_data['flow_rate_vph'])\n",
    "        timestamps.append(current_data['timestamp'])\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if next_state is None:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "      # Create the speed panel animation\n",
    "    create_speed_limit_panel_animation(timestamps, recommended_speeds)\n",
    "    \n",
    "    return timestamps, recommended_speeds, actual_speeds, flow_rates\n",
    "\n",
    "# Modified visualization function with animation\n",
    "def visualize_results_with_animation(timestamps, recommended_speeds, actual_speeds, flow_rates):\n",
    "    # Static visualization (same as before)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot recommended speed limits and actual speeds\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(timestamps, recommended_speeds, 'b-', label='Recommended Speed Limit')\n",
    "    plt.plot(timestamps, actual_speeds, 'r-', label='Actual Average Speed')\n",
    "    plt.ylabel('Speed (km/h)')\n",
    "    plt.title('Continuous VSL Recommendations vs. Actual Speeds')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot flow rates\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(timestamps, flow_rates, 'g-', label='Flow Rate')\n",
    "    plt.ylabel('Flow Rate (vph)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/continuous_vsl_results.png')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Run the complete pipeline\n",
    "def run_continuous_vsl_pipeline():\n",
    "    # Load data\n",
    "    data = load_traffic_data(\"traffic_data_2.csv\")\n",
    "    \n",
    "    # Train the agent\n",
    "    results, agent = train_continuous_vsl_agent(data, episodes=50)\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    timestamps, recommended_speeds, actual_speeds, flow_rates = evaluate_continuous_vsl_agent(data, agent)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results_with_animation(timestamps, recommended_speeds, actual_speeds, flow_rates)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_continuous_vsl_pipeline()\n",
    "    print(\"Continuous VSL implementation completed. Results and animations saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
