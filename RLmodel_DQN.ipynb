{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/50, Avg Reward: 0.3892\n",
      "Episode: 2/50, Avg Reward: 0.3719\n",
      "Episode: 3/50, Avg Reward: 0.3840\n",
      "Episode: 4/50, Avg Reward: 0.3910\n",
      "Episode: 5/50, Avg Reward: 0.3899\n",
      "Episode: 6/50, Avg Reward: 0.3729\n",
      "Episode: 7/50, Avg Reward: 0.3946\n",
      "Episode: 8/50, Avg Reward: 0.3915\n",
      "Episode: 9/50, Avg Reward: 0.4099\n",
      "Episode: 10/50, Avg Reward: 0.3899\n",
      "Episode: 11/50, Avg Reward: 0.4470\n",
      "Episode: 12/50, Avg Reward: 0.3879\n",
      "Episode: 13/50, Avg Reward: 0.3669\n",
      "Episode: 14/50, Avg Reward: 0.3938\n",
      "Episode: 15/50, Avg Reward: 0.3805\n",
      "Episode: 16/50, Avg Reward: 0.4230\n",
      "Episode: 17/50, Avg Reward: 0.4280\n",
      "Episode: 18/50, Avg Reward: 0.4089\n",
      "Episode: 19/50, Avg Reward: 0.3659\n",
      "Episode: 20/50, Avg Reward: 0.3577\n",
      "Episode: 21/50, Avg Reward: 0.4486\n",
      "Episode: 22/50, Avg Reward: 0.3561\n",
      "Episode: 23/50, Avg Reward: 0.4189\n",
      "Episode: 24/50, Avg Reward: 0.4322\n",
      "Episode: 25/50, Avg Reward: 0.3819\n",
      "Episode: 26/50, Avg Reward: 0.4194\n",
      "Episode: 27/50, Avg Reward: 0.3954\n",
      "Episode: 28/50, Avg Reward: 0.3929\n",
      "Episode: 29/50, Avg Reward: 0.4085\n",
      "Episode: 30/50, Avg Reward: 0.3971\n",
      "Episode: 31/50, Avg Reward: 0.3686\n",
      "Episode: 32/50, Avg Reward: 0.3706\n",
      "Episode: 33/50, Avg Reward: 0.3381\n",
      "Episode: 34/50, Avg Reward: 0.4023\n",
      "Episode: 35/50, Avg Reward: 0.3766\n",
      "Episode: 36/50, Avg Reward: 0.3952\n",
      "Episode: 37/50, Avg Reward: 0.3224\n",
      "Episode: 38/50, Avg Reward: 0.4111\n",
      "Episode: 39/50, Avg Reward: 0.3837\n",
      "Episode: 40/50, Avg Reward: 0.3687\n",
      "Episode: 41/50, Avg Reward: 0.4000\n",
      "Episode: 42/50, Avg Reward: 0.4087\n",
      "Episode: 43/50, Avg Reward: 0.3917\n",
      "Episode: 44/50, Avg Reward: 0.4088\n",
      "Episode: 45/50, Avg Reward: 0.3131\n",
      "Episode: 46/50, Avg Reward: 0.4206\n",
      "Episode: 47/50, Avg Reward: 0.3145\n",
      "Episode: 48/50, Avg Reward: 0.3805\n",
      "Episode: 49/50, Avg Reward: 0.3281\n",
      "Episode: 50/50, Avg Reward: 0.3166\n",
      "VSL implementation completed. Results saved to 'vsl_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Load and prepare the data\n",
    "def load_traffic_data(data_path):\n",
    "    # In a real implementation, you'd load from a file\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "# Define the VSL environment\n",
    "class VSLEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        self.speed_limits = [60, 80, 100, 120, 140]  # Available speed limits in km/h\n",
    "        \n",
    "        # Define state space features\n",
    "        self.state_features = ['vehicle_count', 'average_speed_kmh', 'vehicle_density_vpkm']\n",
    "        \n",
    "        # Normalize the features for better learning\n",
    "        self.feature_means = data[self.state_features].mean()\n",
    "        self.feature_stds = data[self.state_features].std()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        if self.current_step >= len(self.data):\n",
    "            return None\n",
    "        \n",
    "        current_data = self.data.iloc[self.current_step][self.state_features]\n",
    "        # Normalize the state\n",
    "        normalized_state = (current_data - self.feature_means) / self.feature_stds\n",
    "        return normalized_state.values.astype(np.float32)  # Ensure float32 type\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            return None, 0, True, {}\n",
    "        \n",
    "        # Get current traffic conditions\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Apply the selected speed limit\n",
    "        selected_speed_limit = self.speed_limits[action]\n",
    "        \n",
    "        # Advance to the next step\n",
    "        self.current_step += 1\n",
    "        next_data = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # Calculate reward based on traffic efficiency and safety\n",
    "        reward = self._calculate_reward(current_data, next_data, selected_speed_limit)\n",
    "        \n",
    "        # Get new state\n",
    "        new_state = self._get_state()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        return new_state, reward, done, {'speed_limit': selected_speed_limit}\n",
    "    \n",
    "    def _calculate_reward(self, current_data, next_data, speed_limit):\n",
    "        # Traffic efficiency component: Reward higher flow rates\n",
    "        flow_reward = next_data['flow_rate_vph'] / 1000  # Normalize\n",
    "        \n",
    "        # Safety component: Penalize if speed is much higher than the limit\n",
    "        speed_compliance = max(0, 1 - max(0, next_data['average_speed_kmh'] - speed_limit) / 50)\n",
    "        \n",
    "        # Stability component: Reward lower variations in speed\n",
    "        speed_stability = 1 / (1 + abs(next_data['average_speed_kmh'] - current_data['average_speed_kmh']))\n",
    "        \n",
    "        # Occupancy component: Reward balanced lane utilization\n",
    "        occupancy_vars = np.var([\n",
    "            next_data['occupancy_lane_1'], \n",
    "            next_data['occupancy_lane_2'], \n",
    "            next_data['occupancy_lane_3']\n",
    "        ])\n",
    "        occupancy_balance = 1 / (1 + occupancy_vars/1000)\n",
    "        \n",
    "        # Combined reward\n",
    "        reward = (0.4 * flow_reward + \n",
    "                  0.3 * speed_compliance + \n",
    "                  0.2 * speed_stability + \n",
    "                  0.1 * occupancy_balance)\n",
    "        \n",
    "        return float(reward)  # Ensure float type\n",
    "\n",
    "# Implement Deep Q-Network agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(np.array([state], dtype=np.float32), verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Process states and next_states to ensure they're float32\n",
    "        states = np.array([i[0] for i in minibatch], dtype=np.float32)\n",
    "        actions = np.array([i[1] for i in minibatch], dtype=np.int32)\n",
    "        rewards = np.array([i[2] for i in minibatch], dtype=np.float32)\n",
    "        next_states = np.array([i[3] for i in minibatch], dtype=np.float32)\n",
    "        dones = np.array([i[4] for i in minibatch], dtype=np.bool_)\n",
    "        \n",
    "        # Check if any next_state is None (end of episode)\n",
    "        # This step is crucial to avoid None values in the array\n",
    "        mask = ~np.array([ns is None for ns in next_states])\n",
    "        if not np.any(mask):\n",
    "            return  # If all next_states are None, exit\n",
    "            \n",
    "        # Calculate target values for fitting\n",
    "        targets = np.zeros((len(minibatch), self.action_size), dtype=np.float32)\n",
    "        \n",
    "        # Predict Q-values for current states\n",
    "        target_f = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        # Predict Q-values for next states where available\n",
    "        if np.any(~dones):\n",
    "            # Filter out None next_states \n",
    "            valid_next_states = next_states[~dones]\n",
    "            if len(valid_next_states) > 0:\n",
    "                next_qs_array = self.model.predict(valid_next_states, verbose=0)\n",
    "                next_qs = np.zeros(len(dones))\n",
    "                next_qs[~dones] = np.max(next_qs_array, axis=1)\n",
    "        \n",
    "        for i, (_, action, reward, _, done) in enumerate(minibatch):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * next_qs[i]\n",
    "            target_f[i][action] = target\n",
    "            \n",
    "        # Train the model\n",
    "        self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# Main training function\n",
    "def train_vsl_agent(data, episodes=100, batch_size=32):\n",
    "    env = VSLEnvironment(data)\n",
    "    state_size = len(env.state_features)\n",
    "    action_size = len(env.speed_limits)\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        'episode': [],\n",
    "        'average_reward': [],\n",
    "        'speed_limits': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        speed_limits_applied = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if next_state is None:  # End of data\n",
    "                break\n",
    "                \n",
    "            speed_limits_applied.append(info['speed_limit'])\n",
    "            total_reward += reward\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "        \n",
    "        # Train the agent on a batch of experiences\n",
    "        if len(agent.memory) >= batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        \n",
    "        # Store results\n",
    "        results['episode'].append(episode)\n",
    "        results['average_reward'].append(total_reward / len(data))\n",
    "        results['speed_limits'].append(speed_limits_applied)\n",
    "        \n",
    "        print(f\"Episode: {episode+1}/{episodes}, Avg Reward: {total_reward/len(data):.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent.save(\"models/vsl_model.weights.h5\")\n",
    "    return results, agent\n",
    "\n",
    "# Evaluate the trained agent\n",
    "def evaluate_vsl_agent(data, agent):\n",
    "    env = VSLEnvironment(data)\n",
    "    state = env.reset()\n",
    "    recommended_speeds = []\n",
    "    actual_speeds = []\n",
    "    flow_rates = []\n",
    "    timestamps = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        speed_limit = env.speed_limits[action]\n",
    "        recommended_speeds.append(speed_limit)\n",
    "        \n",
    "        # Store actual values for comparison\n",
    "        current_data = data.iloc[env.current_step]\n",
    "        actual_speeds.append(current_data['average_speed_kmh'])\n",
    "        flow_rates.append(current_data['flow_rate_vph'])\n",
    "        timestamps.append(current_data['timestamp'])\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if next_state is None:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    return timestamps, recommended_speeds, actual_speeds, flow_rates\n",
    "\n",
    "# Visualize results\n",
    "def visualize_results(timestamps, recommended_speeds, actual_speeds, flow_rates):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot recommended speed limits and actual speeds\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(timestamps, recommended_speeds, 'b-', label='Recommended Speed Limit')\n",
    "    plt.plot(timestamps, actual_speeds, 'r-', label='Actual Average Speed')\n",
    "    plt.ylabel('Speed (km/h)')\n",
    "    plt.title('VSL Recommendations vs. Actual Speeds')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot flow rates\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(timestamps, flow_rates, 'g-', label='Flow Rate')\n",
    "    plt.ylabel('Flow Rate (vph)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vsl_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Run the complete pipeline\n",
    "def run_vsl_pipeline():\n",
    "    # Load data\n",
    "    data = load_traffic_data(\"traffic_data_2.csv\")\n",
    "    \n",
    "    # Train the agent\n",
    "    results, agent = train_vsl_agent(data, episodes=50)\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    timestamps, recommended_speeds, actual_speeds, flow_rates = evaluate_vsl_agent(data, agent)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(timestamps, recommended_speeds, actual_speeds, flow_rates)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_vsl_pipeline()\n",
    "    print(\"VSL implementation completed. Results saved to 'vsl_results.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
