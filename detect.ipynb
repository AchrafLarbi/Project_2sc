{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import time\n",
    "\n",
    "# Load the video\n",
    "video_path = \"test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get FPS of video\n",
    "FPS = cap.get(cv2.CAP_PROP_FPS)  \n",
    "\n",
    "# Load YOLOv8 for vehicle detection\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "tracker = sv.ByteTrack()  # max_age ensures smooth tracking over frames\n",
    "\n",
    "# Calibration: Define real-world meters per pixel (adjust based on your scene)\n",
    "METERS_PER_PIXEL = 0.02  \n",
    "\n",
    "# Dictionary to store previous positions of vehicles\n",
    "previous_positions = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 cars, 245.8ms\n",
      "Speed: 17.0ms preprocess, 245.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m             detected_objects.append([x1, y1, x2, y2])\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Update tracker\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m trackers = \u001b[43mtracker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_with_detections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdetected_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Annotate and track objects\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m trackers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\supervision\\tracker\\byte_tracker\\core.py:108\u001b[39m, in \u001b[36mByteTrack.update_with_detections\u001b[39m\u001b[34m(self, detections)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_with_detections\u001b[39m(\u001b[38;5;28mself\u001b[39m, detections: Detections) -> Detections:\n\u001b[32m     68\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    Updates the tracker with the provided detections and returns the updated\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m    detection results.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m     tensors = np.hstack(\n\u001b[32m    107\u001b[39m         (\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m             \u001b[43mdetections\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxyxy\u001b[49m,\n\u001b[32m    109\u001b[39m             detections.confidence[:, np.newaxis],\n\u001b[32m    110\u001b[39m         )\n\u001b[32m    111\u001b[39m     )\n\u001b[32m    112\u001b[39m     tracks = \u001b[38;5;28mself\u001b[39m.update_with_tensors(tensors=tensors)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tracks) > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'xyxy'"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform detection with YOLO\n",
    "    results = model(frame)\n",
    "\n",
    "    detected_objects = []\n",
    "    \n",
    "    # Process results\n",
    "    def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "        results = model(frame)[0]\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "        return box_annotator.annotate(frame.copy(), detections=detections)\n",
    "\n",
    "    # Update tracker\n",
    "    trackers = tracker.update_with_detections(detections=detected_objects)\n",
    "\n",
    "    # Annotate and track objects\n",
    "    for track in trackers:\n",
    "        track_id = track[4]  # Unique ID assigned to each tracked object\n",
    "        x1, y1, x2, y2 = map(int, track[:4])\n",
    "        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Speed calculation (based on centroid movement)\n",
    "        if track_id in previous_positions:\n",
    "            prev_x, prev_y, prev_time = previous_positions[track_id]\n",
    "            pixel_distance = ((center_x - prev_x) ** 2 + (center_y - prev_y) ** 2) ** 0.5\n",
    "            distance_meters = pixel_distance * METERS_PER_PIXEL\n",
    "            elapsed_time = time.time() - prev_time\n",
    "\n",
    "            if elapsed_time > 0:\n",
    "                speed_mps = distance_meters / elapsed_time\n",
    "                speed_kmh = speed_mps * 3.6\n",
    "                cv2.putText(frame, f\"{int(speed_kmh)} km/h\", (x1, y1 - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        previous_positions[track_id] = (center_x, center_y, time.time())\n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID {track_id}\", (x1 + 10, y1 - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"Vehicle Speed Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 cars, 183.6ms\n",
      "Speed: 9.9ms preprocess, 183.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 198.7ms\n",
      "Speed: 6.1ms preprocess, 198.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 151.0ms\n",
      "Speed: 8.7ms preprocess, 151.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 208.1ms\n",
      "Speed: 10.2ms preprocess, 208.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 175.7ms\n",
      "Speed: 5.1ms preprocess, 175.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 195.4ms\n",
      "Speed: 3.2ms preprocess, 195.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 173.3ms\n",
      "Speed: 5.3ms preprocess, 173.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 182.8ms\n",
      "Speed: 7.6ms preprocess, 182.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 152.8ms\n",
      "Speed: 6.1ms preprocess, 152.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 162.2ms\n",
      "Speed: 7.7ms preprocess, 162.2ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 174.4ms\n",
      "Speed: 7.6ms preprocess, 174.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 143.8ms\n",
      "Speed: 7.1ms preprocess, 143.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 167.0ms\n",
      "Speed: 6.3ms preprocess, 167.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 140.9ms\n",
      "Speed: 6.6ms preprocess, 140.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 131.5ms\n",
      "Speed: 6.9ms preprocess, 131.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 149.5ms\n",
      "Speed: 5.6ms preprocess, 149.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 153.2ms\n",
      "Speed: 5.1ms preprocess, 153.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 203.7ms\n",
      "Speed: 14.5ms preprocess, 203.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 171.5ms\n",
      "Speed: 6.8ms preprocess, 171.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 138.1ms\n",
      "Speed: 6.6ms preprocess, 138.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 161.0ms\n",
      "Speed: 5.1ms preprocess, 161.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 175.9ms\n",
      "Speed: 10.6ms preprocess, 175.9ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 224.9ms\n",
      "Speed: 5.4ms preprocess, 224.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 211.0ms\n",
      "Speed: 5.3ms preprocess, 211.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 389.1ms\n",
      "Speed: 7.5ms preprocess, 389.1ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 179.2ms\n",
      "Speed: 6.7ms preprocess, 179.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 199.6ms\n",
      "Speed: 7.6ms preprocess, 199.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 272.2ms\n",
      "Speed: 11.3ms preprocess, 272.2ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 198.4ms\n",
      "Speed: 7.6ms preprocess, 198.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 177.1ms\n",
      "Speed: 7.1ms preprocess, 177.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 185.3ms\n",
      "Speed: 5.8ms preprocess, 185.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 217.8ms\n",
      "Speed: 5.7ms preprocess, 217.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 171.3ms\n",
      "Speed: 6.0ms preprocess, 171.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 226.9ms\n",
      "Speed: 5.5ms preprocess, 226.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 207.9ms\n",
      "Speed: 7.7ms preprocess, 207.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 173.9ms\n",
      "Speed: 30.1ms preprocess, 173.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 170.5ms\n",
      "Speed: 8.0ms preprocess, 170.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 173.2ms\n",
      "Speed: 8.1ms preprocess, 173.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 201.9ms\n",
      "Speed: 5.9ms preprocess, 201.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 177.1ms\n",
      "Speed: 6.1ms preprocess, 177.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 181.2ms\n",
      "Speed: 7.6ms preprocess, 181.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 171.9ms\n",
      "Speed: 8.0ms preprocess, 171.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 161.0ms\n",
      "Speed: 5.1ms preprocess, 161.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 143.5ms\n",
      "Speed: 4.6ms preprocess, 143.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 174.0ms\n",
      "Speed: 7.5ms preprocess, 174.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 162.0ms\n",
      "Speed: 7.0ms preprocess, 162.0ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 160.9ms\n",
      "Speed: 9.3ms preprocess, 160.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 167.6ms\n",
      "Speed: 4.9ms preprocess, 167.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 165.2ms\n",
      "Speed: 6.2ms preprocess, 165.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 155.9ms\n",
      "Speed: 6.8ms preprocess, 155.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 191.8ms\n",
      "Speed: 9.0ms preprocess, 191.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 168.5ms\n",
      "Speed: 5.6ms preprocess, 168.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 173.1ms\n",
      "Speed: 5.8ms preprocess, 173.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 154.4ms\n",
      "Speed: 4.9ms preprocess, 154.4ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 156.9ms\n",
      "Speed: 6.2ms preprocess, 156.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 201.3ms\n",
      "Speed: 7.2ms preprocess, 201.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 164.8ms\n",
      "Speed: 6.5ms preprocess, 164.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 166.9ms\n",
      "Speed: 8.3ms preprocess, 166.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 140.9ms\n",
      "Speed: 4.8ms preprocess, 140.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 129.2ms\n",
      "Speed: 4.8ms preprocess, 129.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 124.1ms\n",
      "Speed: 4.6ms preprocess, 124.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 127.4ms\n",
      "Speed: 4.5ms preprocess, 127.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 165.3ms\n",
      "Speed: 5.4ms preprocess, 165.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 225.9ms\n",
      "Speed: 7.6ms preprocess, 225.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 181.8ms\n",
      "Speed: 5.4ms preprocess, 181.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 224.1ms\n",
      "Speed: 10.3ms preprocess, 224.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 184.9ms\n",
      "Speed: 6.5ms preprocess, 184.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 142.9ms\n",
      "Speed: 5.8ms preprocess, 142.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 139.6ms\n",
      "Speed: 6.3ms preprocess, 139.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 156.9ms\n",
      "Speed: 7.4ms preprocess, 156.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 136.1ms\n",
      "Speed: 7.6ms preprocess, 136.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 156.8ms\n",
      "Speed: 4.9ms preprocess, 156.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 162.5ms\n",
      "Speed: 4.9ms preprocess, 162.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 139.4ms\n",
      "Speed: 7.5ms preprocess, 139.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 139.1ms\n",
      "Speed: 6.6ms preprocess, 139.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 397.3ms\n",
      "Speed: 16.9ms preprocess, 397.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 141.4ms\n",
      "Speed: 7.0ms preprocess, 141.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 159.4ms\n",
      "Speed: 6.4ms preprocess, 159.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 138.6ms\n",
      "Speed: 8.9ms preprocess, 138.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 143.1ms\n",
      "Speed: 7.0ms preprocess, 143.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 150.9ms\n",
      "Speed: 6.5ms preprocess, 150.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 213.5ms\n",
      "Speed: 6.5ms preprocess, 213.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 164.8ms\n",
      "Speed: 7.1ms preprocess, 164.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 154.3ms\n",
      "Speed: 5.2ms preprocess, 154.3ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 151.7ms\n",
      "Speed: 6.0ms preprocess, 151.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 146.2ms\n",
      "Speed: 5.6ms preprocess, 146.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 166.6ms\n",
      "Speed: 6.0ms preprocess, 166.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 176.5ms\n",
      "Speed: 5.4ms preprocess, 176.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 178.0ms\n",
      "Speed: 7.2ms preprocess, 178.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 158.0ms\n",
      "Speed: 6.0ms preprocess, 158.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 212.5ms\n",
      "Speed: 44.6ms preprocess, 212.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 182.0ms\n",
      "Speed: 7.9ms preprocess, 182.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 196.3ms\n",
      "Speed: 10.8ms preprocess, 196.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 213.2ms\n",
      "Speed: 25.4ms preprocess, 213.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 motorcycle, 170.8ms\n",
      "Speed: 5.1ms preprocess, 170.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 motorcycles, 1 truck, 191.0ms\n",
      "Speed: 6.1ms preprocess, 191.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 2 motorcycles, 1 truck, 194.8ms\n",
      "Speed: 13.2ms preprocess, 194.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 1 motorcycle, 194.9ms\n",
      "Speed: 11.2ms preprocess, 194.9ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 3 motorcycles, 203.1ms\n",
      "Speed: 5.6ms preprocess, 203.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 motorcycles, 151.0ms\n",
      "Speed: 5.2ms preprocess, 151.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 153.4ms\n",
      "Speed: 6.5ms preprocess, 153.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 273.2ms\n",
      "Speed: 4.9ms preprocess, 273.2ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 motorcycles, 153.4ms\n",
      "Speed: 5.0ms preprocess, 153.4ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 motorcycle, 144.4ms\n",
      "Speed: 5.4ms preprocess, 144.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 2 motorcycles, 174.0ms\n",
      "Speed: 11.9ms preprocess, 174.0ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 2 motorcycles, 154.6ms\n",
      "Speed: 7.1ms preprocess, 154.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 motorcycle, 142.9ms\n",
      "Speed: 8.0ms preprocess, 142.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 motorcycle, 144.5ms\n",
      "Speed: 7.8ms preprocess, 144.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 146.8ms\n",
      "Speed: 8.6ms preprocess, 146.8ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 motorcycle, 179.1ms\n",
      "Speed: 5.3ms preprocess, 179.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 206.4ms\n",
      "Speed: 7.4ms preprocess, 206.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 152.5ms\n",
      "Speed: 7.1ms preprocess, 152.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 151.6ms\n",
      "Speed: 4.7ms preprocess, 151.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 169.3ms\n",
      "Speed: 6.6ms preprocess, 169.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9 cars, 148.3ms\n",
      "Speed: 12.6ms preprocess, 148.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 155.2ms\n",
      "Speed: 6.3ms preprocess, 155.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 159.0ms\n",
      "Speed: 6.8ms preprocess, 159.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 158.1ms\n",
      "Speed: 8.3ms preprocess, 158.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 133.9ms\n",
      "Speed: 5.0ms preprocess, 133.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 151.2ms\n",
      "Speed: 7.3ms preprocess, 151.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 3 motorcycles, 158.9ms\n",
      "Speed: 4.6ms preprocess, 158.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 157.5ms\n",
      "Speed: 5.4ms preprocess, 157.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 139.9ms\n",
      "Speed: 5.3ms preprocess, 139.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 motorcycle, 178.2ms\n",
      "Speed: 7.8ms preprocess, 178.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 motorcycle, 146.1ms\n",
      "Speed: 7.3ms preprocess, 146.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10 cars, 3 motorcycles, 155.7ms\n",
      "Speed: 6.7ms preprocess, 155.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 motorcycle, 159.8ms\n",
      "Speed: 5.0ms preprocess, 159.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 motorcycle, 149.3ms\n",
      "Speed: 5.1ms preprocess, 149.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 motorcycle, 132.6ms\n",
      "Speed: 4.8ms preprocess, 132.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 139.3ms\n",
      "Speed: 6.2ms preprocess, 139.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 160.2ms\n",
      "Speed: 7.5ms preprocess, 160.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 146.5ms\n",
      "Speed: 5.0ms preprocess, 146.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 150.5ms\n",
      "Speed: 7.1ms preprocess, 150.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 151.6ms\n",
      "Speed: 6.1ms preprocess, 151.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 157.5ms\n",
      "Speed: 7.8ms preprocess, 157.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 motorcycle, 159.6ms\n",
      "Speed: 6.3ms preprocess, 159.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 165.0ms\n",
      "Speed: 5.4ms preprocess, 165.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 135.8ms\n",
      "Speed: 5.9ms preprocess, 135.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 129.8ms\n",
      "Speed: 7.1ms preprocess, 129.8ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 152.0ms\n",
      "Speed: 4.9ms preprocess, 152.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 142.9ms\n",
      "Speed: 6.8ms preprocess, 142.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 143.5ms\n",
      "Speed: 5.0ms preprocess, 143.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 144.0ms\n",
      "Speed: 5.7ms preprocess, 144.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 178.9ms\n",
      "Speed: 4.9ms preprocess, 178.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 148.8ms\n",
      "Speed: 6.7ms preprocess, 148.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 137.5ms\n",
      "Speed: 7.1ms preprocess, 137.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 154.8ms\n",
      "Speed: 8.7ms preprocess, 154.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 197.2ms\n",
      "Speed: 18.4ms preprocess, 197.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 174.9ms\n",
      "Speed: 7.1ms preprocess, 174.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 170.2ms\n",
      "Speed: 8.4ms preprocess, 170.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 146.0ms\n",
      "Speed: 4.9ms preprocess, 146.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 161.7ms\n",
      "Speed: 4.7ms preprocess, 161.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 147.7ms\n",
      "Speed: 4.5ms preprocess, 147.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 143.9ms\n",
      "Speed: 7.3ms preprocess, 143.9ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 150.5ms\n",
      "Speed: 6.3ms preprocess, 150.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 140.9ms\n",
      "Speed: 4.8ms preprocess, 140.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 171.1ms\n",
      "Speed: 21.1ms preprocess, 171.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 157.6ms\n",
      "Speed: 4.8ms preprocess, 157.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 147.0ms\n",
      "Speed: 7.5ms preprocess, 147.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 147.6ms\n",
      "Speed: 4.9ms preprocess, 147.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 147.5ms\n",
      "Speed: 4.6ms preprocess, 147.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 168.1ms\n",
      "Speed: 8.0ms preprocess, 168.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 169.9ms\n",
      "Speed: 6.4ms preprocess, 169.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 200.3ms\n",
      "Speed: 4.6ms preprocess, 200.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 142.7ms\n",
      "Speed: 7.3ms preprocess, 142.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 151.0ms\n",
      "Speed: 6.9ms preprocess, 151.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 151.3ms\n",
      "Speed: 5.1ms preprocess, 151.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 188.6ms\n",
      "Speed: 5.4ms preprocess, 188.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 197.9ms\n",
      "Speed: 8.1ms preprocess, 197.9ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 150.7ms\n",
      "Speed: 6.0ms preprocess, 150.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 177.7ms\n",
      "Speed: 5.0ms preprocess, 177.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 181.1ms\n",
      "Speed: 6.8ms preprocess, 181.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 164.1ms\n",
      "Speed: 6.4ms preprocess, 164.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 326.8ms\n",
      "Speed: 16.0ms preprocess, 326.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 158.5ms\n",
      "Speed: 32.3ms preprocess, 158.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 148.4ms\n",
      "Speed: 4.7ms preprocess, 148.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 147.9ms\n",
      "Speed: 5.3ms preprocess, 147.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 179.7ms\n",
      "Speed: 4.9ms preprocess, 179.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 202.8ms\n",
      "Speed: 7.0ms preprocess, 202.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 306.1ms\n",
      "Speed: 7.3ms preprocess, 306.1ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 196.1ms\n",
      "Speed: 45.1ms preprocess, 196.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 277.0ms\n",
      "Speed: 5.6ms preprocess, 277.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 182.5ms\n",
      "Speed: 6.0ms preprocess, 182.5ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 179.8ms\n",
      "Speed: 6.8ms preprocess, 179.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 228.1ms\n",
      "Speed: 4.9ms preprocess, 228.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 155.5ms\n",
      "Speed: 6.1ms preprocess, 155.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 141.7ms\n",
      "Speed: 7.0ms preprocess, 141.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 151.0ms\n",
      "Speed: 8.2ms preprocess, 151.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 175.5ms\n",
      "Speed: 7.7ms preprocess, 175.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 153.9ms\n",
      "Speed: 6.2ms preprocess, 153.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 153.5ms\n",
      "Speed: 6.0ms preprocess, 153.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 150.9ms\n",
      "Speed: 6.4ms preprocess, 150.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 148.4ms\n",
      "Speed: 6.8ms preprocess, 148.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 2 trucks, 145.8ms\n",
      "Speed: 5.0ms preprocess, 145.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 truck, 142.4ms\n",
      "Speed: 6.7ms preprocess, 142.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 156.4ms\n",
      "Speed: 5.7ms preprocess, 156.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 147.4ms\n",
      "Speed: 6.8ms preprocess, 147.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 213.5ms\n",
      "Speed: 4.8ms preprocess, 213.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 164.3ms\n",
      "Speed: 4.9ms preprocess, 164.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 bus, 157.0ms\n",
      "Speed: 21.9ms preprocess, 157.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 143.7ms\n",
      "Speed: 6.9ms preprocess, 143.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 174.5ms\n",
      "Speed: 6.3ms preprocess, 174.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 truck, 159.1ms\n",
      "Speed: 7.4ms preprocess, 159.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 191.1ms\n",
      "Speed: 7.0ms preprocess, 191.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 152.7ms\n",
      "Speed: 6.0ms preprocess, 152.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 170.1ms\n",
      "Speed: 5.4ms preprocess, 170.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 191.3ms\n",
      "Speed: 5.3ms preprocess, 191.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 bus, 1 truck, 139.4ms\n",
      "Speed: 6.6ms preprocess, 139.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 151.9ms\n",
      "Speed: 7.5ms preprocess, 151.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 241.3ms\n",
      "Speed: 9.1ms preprocess, 241.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 truck, 143.3ms\n",
      "Speed: 5.1ms preprocess, 143.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 150.3ms\n",
      "Speed: 5.2ms preprocess, 150.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 truck, 142.7ms\n",
      "Speed: 6.0ms preprocess, 142.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 145.9ms\n",
      "Speed: 5.7ms preprocess, 145.9ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 156.2ms\n",
      "Speed: 8.4ms preprocess, 156.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 13 cars, 2 motorcycles, 1 truck, 131.7ms\n",
      "Speed: 6.2ms preprocess, 131.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 11 cars, 1 motorcycle, 1 truck, 143.3ms\n",
      "Speed: 8.0ms preprocess, 143.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 11 cars, 2 motorcycles, 1 truck, 159.8ms\n",
      "Speed: 6.7ms preprocess, 159.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 bus, 1 truck, 162.3ms\n",
      "Speed: 6.1ms preprocess, 162.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 bus, 1 truck, 146.4ms\n",
      "Speed: 4.6ms preprocess, 146.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 2 motorcycles, 1 bus, 1 truck, 151.1ms\n",
      "Speed: 5.5ms preprocess, 151.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 175.4ms\n",
      "Speed: 5.3ms preprocess, 175.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 204.8ms\n",
      "Speed: 9.2ms preprocess, 204.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 motorcycle, 1 bus, 174.6ms\n",
      "Speed: 5.4ms preprocess, 174.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 205.7ms\n",
      "Speed: 22.5ms preprocess, 205.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 2 motorcycles, 1 truck, 206.2ms\n",
      "Speed: 18.1ms preprocess, 206.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 motorcycle, 1 bus, 1 truck, 235.5ms\n",
      "Speed: 30.3ms preprocess, 235.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 263.1ms\n",
      "Speed: 8.0ms preprocess, 263.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 203.5ms\n",
      "Speed: 17.9ms preprocess, 203.5ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 motorcycle, 1 bus, 1 truck, 175.6ms\n",
      "Speed: 7.6ms preprocess, 175.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 2 motorcycles, 1 bus, 1 truck, 153.5ms\n",
      "Speed: 6.8ms preprocess, 153.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 motorcycle, 1 truck, 144.3ms\n",
      "Speed: 5.4ms preprocess, 144.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_annotator.annotate(annotated_frame, detections=detections)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Process the video using Supervision library\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43msv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your input video path\u001b[39;49;00m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult_traffic.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Output video path\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\supervision\\utils\\video.py:230\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(source_path, target_path, callback)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m VideoSink(target_path=target_path, video_info=source_video_info) \u001b[38;5;28;01mas\u001b[39;00m sink:\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m index, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[32m    228\u001b[39m         get_video_frames_generator(source_path=source_path)\n\u001b[32m    229\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         result_frame = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m         sink.write_frame(frame=result_frame)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mcallback\u001b[39m\u001b[34m(frame, _)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcallback\u001b[39m(frame: np.ndarray, _: \u001b[38;5;28mint\u001b[39m) -> np.ndarray:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Perform object detection with YOLOv8\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Convert results to Supervision Detections format\u001b[39;00m\n\u001b[32m     25\u001b[39m     detections = sv.Detections.from_ultralytics(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:182\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    155\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    156\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    157\u001b[39m     **kwargs: Any,\n\u001b[32m    158\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    159\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:550\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:214\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = gen.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:323\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:171\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    166\u001b[39m visualize = (\n\u001b[32m    167\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    170\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:568\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    154\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m y.extend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:476\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    475\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28mself\u001b[39m.cv1(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:91\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Load YOLOv8 model (ensure the model path is correct)\n",
    "tracker = sv.ByteTrack()  # Using ByteTrack tracker for object tracking\n",
    "box_annotator = sv.BoundingBoxAnnotator()  # Annotator for bounding boxes\n",
    "label_annotator = sv.LabelAnnotator()  # Annotator for labels (IDs, class names)\n",
    "trace_annotator = sv.TraceAnnotator()  # Annotator for tracking traces\n",
    "\n",
    "# Initialize previous positions dictionary for speed calculation\n",
    "previous_positions = {}\n",
    "\n",
    "# Calibration factor (adjust based on your setup)\n",
    "METERS_PER_PIXEL = 0.02  # Convert pixels to meters\n",
    "\n",
    "def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "    # Perform object detection with YOLOv8\n",
    "    results = model(frame)[0]\n",
    "    \n",
    "    # Convert results to Supervision Detections format\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    \n",
    "    # Update the tracker with new detections\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "    \n",
    "    # Prepare labels (ID and class name)\n",
    "    labels = [\n",
    "        f\"#{tracker_id} {results.names[class_id]}\"\n",
    "        for class_id, tracker_id\n",
    "        in zip(detections.class_id, detections.tracker_id)\n",
    "    ]\n",
    "    \n",
    "    # Annotate bounding boxes and labels on the frame\n",
    "    annotated_frame = box_annotator.annotate(frame.copy(), detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(annotated_frame, detections=detections, labels=labels)\n",
    "    \n",
    "    # Speed calculation and annotation\n",
    "    for tracker_id, (x1, y1, x2, y2) in zip(detections.tracker_id, detections.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # Speed calculation based on previous position\n",
    "        if tracker_id in previous_positions:\n",
    "            prev_x, prev_y, prev_time = previous_positions[tracker_id]\n",
    "            pixel_distance = ((center_x - prev_x) ** 2 + (center_y - prev_y) ** 2) ** 0.5\n",
    "            distance_meters = pixel_distance * METERS_PER_PIXEL\n",
    "            elapsed_time = time.time() - prev_time\n",
    "\n",
    "            if elapsed_time > 0:\n",
    "                speed_mps = distance_meters / elapsed_time\n",
    "                speed_kmh = speed_mps * 3.6  # Convert from meters per second to kilometers per hour\n",
    "                cv2.putText(annotated_frame, f\"{int(speed_kmh)} km/h\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Store current position and time for next frame\n",
    "        previous_positions[tracker_id] = (center_x, center_y, time.time())\n",
    "\n",
    "    # Annotate the traces of the objects (for visual tracking of movement)\n",
    "    return trace_annotator.annotate(annotated_frame, detections=detections)\n",
    "\n",
    "# Process the video using Supervision library\n",
    "sv.process_video(\n",
    "    source_path=\"test.mp4\",  # Your input video path\n",
    "    target_path=\"result_traffic.mp4\",  # Output video path\n",
    "    callback=callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 cars, 1 truck, 711.4ms\n",
      "Speed: 3.9ms preprocess, 711.4ms inference, 151.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 5 crossed start line.\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 111.1ms\n",
      "Speed: 6.2ms preprocess, 111.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 106.5ms\n",
      "Speed: 5.8ms preprocess, 106.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 116.8ms\n",
      "Speed: 3.8ms preprocess, 116.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 98.5ms\n",
      "Speed: 4.3ms preprocess, 98.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 102.8ms\n",
      "Speed: 5.5ms preprocess, 102.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 98.3ms\n",
      "Speed: 4.4ms preprocess, 98.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 bus, 1 truck, 98.2ms\n",
      "Speed: 6.3ms preprocess, 98.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 124.9ms\n",
      "Speed: 6.5ms preprocess, 124.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 bus, 1 truck, 130.7ms\n",
      "Speed: 4.5ms preprocess, 130.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 bus, 1 truck, 118.3ms\n",
      "Speed: 5.2ms preprocess, 118.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 bus, 1 truck, 100.7ms\n",
      "Speed: 4.8ms preprocess, 100.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 104.3ms\n",
      "Speed: 5.6ms preprocess, 104.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 92.1ms\n",
      "Speed: 7.9ms preprocess, 92.1ms inference, 20.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 bus, 1 truck, 92.1ms\n",
      "Speed: 3.4ms preprocess, 92.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 92.5ms\n",
      "Speed: 6.6ms preprocess, 92.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 96.4ms\n",
      "Speed: 3.6ms preprocess, 96.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 86.6ms\n",
      "Speed: 5.5ms preprocess, 86.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 93.3ms\n",
      "Speed: 3.3ms preprocess, 93.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 90.4ms\n",
      "Speed: 4.5ms preprocess, 90.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 94.6ms\n",
      "Speed: 3.9ms preprocess, 94.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 84.8ms\n",
      "Speed: 4.8ms preprocess, 84.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 87.0ms\n",
      "Speed: 5.2ms preprocess, 87.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 92.7ms\n",
      "Speed: 3.9ms preprocess, 92.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 95.1ms\n",
      "Speed: 5.0ms preprocess, 95.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 131.2ms\n",
      "Speed: 3.6ms preprocess, 131.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 144.4ms\n",
      "Speed: 5.9ms preprocess, 144.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 91.7ms\n",
      "Speed: 4.2ms preprocess, 91.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 88.6ms\n",
      "Speed: 5.9ms preprocess, 88.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 91.9ms\n",
      "Speed: 3.4ms preprocess, 91.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 103.2ms\n",
      "Speed: 7.2ms preprocess, 103.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 105.1ms\n",
      "Speed: 7.7ms preprocess, 105.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 94.9ms\n",
      "Speed: 3.7ms preprocess, 94.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 89.8ms\n",
      "Speed: 6.8ms preprocess, 89.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 trucks, 96.6ms\n",
      "Speed: 3.8ms preprocess, 96.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 89.0ms\n",
      "Speed: 5.1ms preprocess, 89.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 100.5ms\n",
      "Speed: 3.9ms preprocess, 100.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 5 crossed end line.\n",
      "Car 5 speed: 1 km/h\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 93.4ms\n",
      "Speed: 4.1ms preprocess, 93.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 99.4ms\n",
      "Speed: 7.1ms preprocess, 99.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 106.9ms\n",
      "Speed: 4.1ms preprocess, 106.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 17 crossed start line.\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 95.3ms\n",
      "Speed: 6.9ms preprocess, 95.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 96.6ms\n",
      "Speed: 4.2ms preprocess, 96.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 101.2ms\n",
      "Speed: 3.9ms preprocess, 101.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 101.0ms\n",
      "Speed: 6.8ms preprocess, 101.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 102.4ms\n",
      "Speed: 6.7ms preprocess, 102.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 95.1ms\n",
      "Speed: 3.9ms preprocess, 95.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 93.4ms\n",
      "Speed: 4.4ms preprocess, 93.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 87.6ms\n",
      "Speed: 6.3ms preprocess, 87.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 88.7ms\n",
      "Speed: 4.0ms preprocess, 88.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 92.2ms\n",
      "Speed: 3.5ms preprocess, 92.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 86.7ms\n",
      "Speed: 5.0ms preprocess, 86.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 99.8ms\n",
      "Speed: 3.7ms preprocess, 99.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 14 crossed start line.\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 96.5ms\n",
      "Speed: 4.1ms preprocess, 96.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 102.8ms\n",
      "Speed: 5.6ms preprocess, 102.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 97.3ms\n",
      "Speed: 3.5ms preprocess, 97.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 26 crossed start line.\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 91.1ms\n",
      "Speed: 4.4ms preprocess, 91.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 86.5ms\n",
      "Speed: 7.8ms preprocess, 86.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 24 crossed start line.\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 90.0ms\n",
      "Speed: 3.9ms preprocess, 90.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 92.4ms\n",
      "Speed: 7.0ms preprocess, 92.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 94.9ms\n",
      "Speed: 4.1ms preprocess, 94.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 93.3ms\n",
      "Speed: 4.6ms preprocess, 93.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 90.3ms\n",
      "Speed: 6.7ms preprocess, 90.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 95.0ms\n",
      "Speed: 3.8ms preprocess, 95.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 89.0ms\n",
      "Speed: 5.8ms preprocess, 89.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 97.0ms\n",
      "Speed: 4.2ms preprocess, 97.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 109.9ms\n",
      "Speed: 5.9ms preprocess, 109.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 90.2ms\n",
      "Speed: 7.6ms preprocess, 90.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 93.1ms\n",
      "Speed: 2.9ms preprocess, 93.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 90.4ms\n",
      "Speed: 6.6ms preprocess, 90.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 92.1ms\n",
      "Speed: 4.2ms preprocess, 92.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 103.2ms\n",
      "Speed: 6.7ms preprocess, 103.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 27 crossed start line.\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 87.5ms\n",
      "Speed: 2.5ms preprocess, 87.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 97.0ms\n",
      "Speed: 3.8ms preprocess, 97.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 91.5ms\n",
      "Speed: 4.7ms preprocess, 91.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 90.6ms\n",
      "Speed: 6.8ms preprocess, 90.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 92.2ms\n",
      "Speed: 3.7ms preprocess, 92.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 trucks, 89.9ms\n",
      "Speed: 6.6ms preprocess, 89.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 1 crossed start line.\n",
      "\n",
      "0: 384x640 11 cars, 98.1ms\n",
      "Speed: 3.4ms preprocess, 98.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 92.9ms\n",
      "Speed: 3.8ms preprocess, 92.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 89.2ms\n",
      "Speed: 7.5ms preprocess, 89.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 85.1ms\n",
      "Speed: 4.0ms preprocess, 85.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 94.0ms\n",
      "Speed: 2.7ms preprocess, 94.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 trucks, 89.4ms\n",
      "Speed: 5.4ms preprocess, 89.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 92.6ms\n",
      "Speed: 3.8ms preprocess, 92.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 92.5ms\n",
      "Speed: 5.9ms preprocess, 92.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 95.6ms\n",
      "Speed: 3.6ms preprocess, 95.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 97.4ms\n",
      "Speed: 5.3ms preprocess, 97.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 94.0ms\n",
      "Speed: 5.9ms preprocess, 94.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 99.5ms\n",
      "Speed: 3.8ms preprocess, 99.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 102.5ms\n",
      "Speed: 3.9ms preprocess, 102.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 99.9ms\n",
      "Speed: 6.9ms preprocess, 99.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 101.4ms\n",
      "Speed: 4.4ms preprocess, 101.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 94.3ms\n",
      "Speed: 4.2ms preprocess, 94.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 95.5ms\n",
      "Speed: 5.2ms preprocess, 95.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 87.3ms\n",
      "Speed: 5.9ms preprocess, 87.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 96.2ms\n",
      "Speed: 4.7ms preprocess, 96.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 86.5ms\n",
      "Speed: 5.2ms preprocess, 86.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 88.5ms\n",
      "Speed: 3.1ms preprocess, 88.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 bus, 1 truck, 98.1ms\n",
      "Speed: 3.6ms preprocess, 98.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 bus, 2 trucks, 93.5ms\n",
      "Speed: 3.8ms preprocess, 93.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 3 trucks, 104.6ms\n",
      "Speed: 5.6ms preprocess, 104.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 3 trucks, 100.5ms\n",
      "Speed: 4.0ms preprocess, 100.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 14 crossed end line.\n",
      "Car 14 speed: 1 km/h\n",
      "\n",
      "0: 384x640 7 cars, 4 trucks, 99.8ms\n",
      "Speed: 4.2ms preprocess, 99.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 3 trucks, 98.7ms\n",
      "Speed: 4.2ms preprocess, 98.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 5 trucks, 100.6ms\n",
      "Speed: 3.8ms preprocess, 100.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 141.9ms\n",
      "Speed: 4.2ms preprocess, 141.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 124.5ms\n",
      "Speed: 5.8ms preprocess, 124.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 3 trucks, 93.3ms\n",
      "Speed: 6.4ms preprocess, 93.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 bus, 2 trucks, 95.5ms\n",
      "Speed: 4.2ms preprocess, 95.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 112.8ms\n",
      "Speed: 5.1ms preprocess, 112.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 103.4ms\n",
      "Speed: 7.6ms preprocess, 103.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 100.5ms\n",
      "Speed: 5.0ms preprocess, 100.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 trucks, 94.5ms\n",
      "Speed: 3.8ms preprocess, 94.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 3 crossed start line.\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 88.7ms\n",
      "Speed: 5.4ms preprocess, 88.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 100.6ms\n",
      "Speed: 3.7ms preprocess, 100.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 90.4ms\n",
      "Speed: 4.9ms preprocess, 90.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 trucks, 102.5ms\n",
      "Speed: 5.5ms preprocess, 102.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 102.8ms\n",
      "Speed: 5.2ms preprocess, 102.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 3 trucks, 104.9ms\n",
      "Speed: 6.1ms preprocess, 104.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 2 trucks, 92.4ms\n",
      "Speed: 4.1ms preprocess, 92.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 2 trucks, 95.1ms\n",
      "Speed: 6.0ms preprocess, 95.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 2 trucks, 94.6ms\n",
      "Speed: 6.9ms preprocess, 94.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 43 crossed start line.\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 3 trucks, 105.4ms\n",
      "Speed: 4.7ms preprocess, 105.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 2 trucks, 91.0ms\n",
      "Speed: 5.6ms preprocess, 91.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 94.6ms\n",
      "Speed: 6.5ms preprocess, 94.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 24 crossed end line.\n",
      "Car 24 speed: 0 km/h\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 99.0ms\n",
      "Speed: 3.9ms preprocess, 99.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 96.7ms\n",
      "Speed: 4.2ms preprocess, 96.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 2 trucks, 93.2ms\n",
      "Speed: 4.6ms preprocess, 93.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 27 crossed end line.\n",
      "Car 27 speed: 0 km/h\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 2 trucks, 91.7ms\n",
      "Speed: 6.9ms preprocess, 91.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 17 crossed end line.\n",
      "Car 17 speed: 0 km/h\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 2 trucks, 102.4ms\n",
      "Speed: 3.7ms preprocess, 102.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 97.3ms\n",
      "Speed: 3.6ms preprocess, 97.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 trucks, 94.5ms\n",
      "Speed: 3.9ms preprocess, 94.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 trucks, 94.7ms\n",
      "Speed: 5.4ms preprocess, 94.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 trucks, 98.2ms\n",
      "Speed: 5.0ms preprocess, 98.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 trucks, 108.3ms\n",
      "Speed: 3.8ms preprocess, 108.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 3 trucks, 92.7ms\n",
      "Speed: 6.8ms preprocess, 92.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 3 trucks, 95.9ms\n",
      "Speed: 5.0ms preprocess, 95.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 3 trucks, 97.7ms\n",
      "Speed: 4.2ms preprocess, 97.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 15 crossed start line.\n",
      "\n",
      "0: 384x640 12 cars, 3 trucks, 93.9ms\n",
      "Speed: 5.5ms preprocess, 93.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 3 trucks, 106.5ms\n",
      "Speed: 4.2ms preprocess, 106.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 2 trucks, 90.8ms\n",
      "Speed: 7.0ms preprocess, 90.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 99.6ms\n",
      "Speed: 3.5ms preprocess, 99.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 3 trucks, 96.1ms\n",
      "Speed: 3.9ms preprocess, 96.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 39 crossed start line.\n",
      "\n",
      "0: 384x640 13 cars, 3 trucks, 96.6ms\n",
      "Speed: 5.6ms preprocess, 96.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 3 trucks, 97.6ms\n",
      "Speed: 6.8ms preprocess, 97.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 3 trucks, 109.5ms\n",
      "Speed: 7.8ms preprocess, 109.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 4 trucks, 97.2ms\n",
      "Speed: 5.8ms preprocess, 97.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 4 trucks, 99.2ms\n",
      "Speed: 7.1ms preprocess, 99.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Car 13 crossed start line.\n",
      "\n",
      "0: 384x640 12 cars, 4 trucks, 107.7ms\n",
      "Speed: 5.7ms preprocess, 107.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 4 trucks, 98.2ms\n",
      "Speed: 2.8ms preprocess, 98.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 4 trucks, 196.3ms\n",
      "Speed: 4.3ms preprocess, 196.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 4 trucks, 124.8ms\n",
      "Speed: 4.9ms preprocess, 124.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 4 trucks, 127.2ms\n",
      "Speed: 4.4ms preprocess, 127.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 4 trucks, 115.2ms\n",
      "Speed: 7.3ms preprocess, 115.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 5 trucks, 112.5ms\n",
      "Speed: 3.7ms preprocess, 112.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 1 bus, 4 trucks, 96.4ms\n",
      "Speed: 4.1ms preprocess, 96.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 bus, 4 trucks, 102.5ms\n",
      "Speed: 4.3ms preprocess, 102.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 3 trucks, 97.3ms\n",
      "Speed: 5.6ms preprocess, 97.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 5 trucks, 99.2ms\n",
      "Speed: 6.3ms preprocess, 99.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 3 trucks, 88.5ms\n",
      "Speed: 5.8ms preprocess, 88.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 3 trucks, 101.1ms\n",
      "Speed: 4.1ms preprocess, 101.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Perform object detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Convert results to Supervision Detections format\u001b[39;00m\n\u001b[32m     37\u001b[39m detections = sv.Detections.from_ultralytics(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:182\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    155\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    156\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    157\u001b[39m     **kwargs: Any,\n\u001b[32m    158\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    159\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:550\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:214\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = gen.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:323\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:171\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    166\u001b[39m visualize = (\n\u001b[32m    167\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    170\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:568\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    154\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:302\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    300\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    301\u001b[39m y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:91\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\pc\\OneDrive\\Documents\\4cp\\S2\\Project_2cs\\code\\myexample\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Ensure this path is correct\n",
    "tracker = sv.ByteTrack()  # Using ByteTrack tracker\n",
    "box_annotator = sv.BoundingBoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "trace_annotator = sv.TraceAnnotator()\n",
    "\n",
    "# Calibration factor (adjust based on your setup)\n",
    "METERS_PER_PIXEL = 0.02  # Conversion from pixels to meters\n",
    "\n",
    "# Define two lines (adjust based on the video)\n",
    "start_line_y = 300  # Start line position (y-coordinate)\n",
    "end_line_y = 400  # End line position (y-coordinate)\n",
    "\n",
    "# Initialize previous positions and timestamps\n",
    "cross_times = {}  # Track car crossing times\n",
    "\n",
    "# OpenCV VideoCapture for reading the video file\n",
    "cap = cv2.VideoCapture(\"test2.mp4\")  # Replace with your video file path\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Perform object detection\n",
    "    results = model(frame)[0]\n",
    "    \n",
    "    # Convert results to Supervision Detections format\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    \n",
    "    # Update tracker with new detections\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "    \n",
    "    # Prepare labels\n",
    "    labels = [\n",
    "        f\"#{tracker_id} {results.names[class_id]}\"\n",
    "        for class_id, tracker_id in zip(detections.class_id, detections.tracker_id)\n",
    "    ]\n",
    "    \n",
    "    # Annotate bounding boxes and labels\n",
    "    annotated_frame = box_annotator.annotate(frame.copy(), detections=detections)\n",
    "    annotated_frame = label_annotator.annotate(annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    # Draw start and end lines on the frame\n",
    "    cv2.line(annotated_frame, (0, start_line_y), (frame.shape[1], start_line_y), (0, 255, 0), 2)\n",
    "    cv2.line(annotated_frame, (0, end_line_y), (frame.shape[1], end_line_y), (0, 255, 0), 2)\n",
    "\n",
    "    # Process each tracked object\n",
    "    for tracker_id, (x1, y1, x2, y2) in zip(detections.tracker_id, detections.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        if tracker_id not in cross_times:\n",
    "            cross_times[tracker_id] = {\"start\": None, \"end\": None}\n",
    "\n",
    "        # If car crosses the start line\n",
    "        if cross_times[tracker_id][\"start\"] is None and start_line_y - 10 < center_y < start_line_y + 10:\n",
    "            cross_times[tracker_id][\"start\"] = time.time()\n",
    "            print(f\"Car {tracker_id} crossed start line.\")\n",
    "\n",
    "        # If car crosses the end line\n",
    "        if cross_times[tracker_id][\"start\"] is not None and cross_times[tracker_id][\"end\"] is None:\n",
    "            if end_line_y - 10 < center_y < end_line_y + 10:\n",
    "                cross_times[tracker_id][\"end\"] = time.time()\n",
    "                print(f\"Car {tracker_id} crossed end line.\")\n",
    "\n",
    "                # Calculate time to cross lines\n",
    "                start_time = cross_times[tracker_id][\"start\"]\n",
    "                end_time = cross_times[tracker_id][\"end\"]\n",
    "                time_to_cross = end_time - start_time\n",
    "\n",
    "                if time_to_cross > 0:\n",
    "                    # Calculate the distance between the lines (in meters)\n",
    "                    distance_pixels = abs(end_line_y - start_line_y)\n",
    "                    distance_meters = distance_pixels * METERS_PER_PIXEL\n",
    "\n",
    "                    # Calculate speed (in meters per second)\n",
    "                    speed_mps = distance_meters / time_to_cross\n",
    "                    speed_kmh = speed_mps * 3.6  # Convert to km/h\n",
    "\n",
    "                    # Annotate speed on the frame\n",
    "                    speed_text = f\"{int(speed_kmh)} km/h\"\n",
    "                    cv2.putText(annotated_frame, speed_text, (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                    print(f\"Car {tracker_id} speed: {int(speed_kmh)} km/h\")\n",
    "\n",
    "        # Reset crossing times once car has passed both lines\n",
    "        if cross_times[tracker_id][\"end\"] is not None:\n",
    "            cross_times[tracker_id][\"start\"] = None\n",
    "            cross_times[tracker_id][\"end\"] = None\n",
    "\n",
    "    # Annotate object traces\n",
    "    annotated_frame = trace_annotator.annotate(annotated_frame, detections=detections)\n",
    "    \n",
    "    # Show the frame with annotations\n",
    "    cv2.imshow(\"Speed Detection\", annotated_frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
